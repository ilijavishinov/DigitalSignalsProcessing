{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Why is sistematization important?\n",
    "\n",
    "The annotation files are not consistent between databases,\n",
    "and even within the cudb database itself.\n",
    "\n",
    "For example, in some annotation files the start of a VF episode is annotated by \"+, (VF\",\n",
    "and somewhere only by '\\[' for start and '\\]' for end.\n",
    "Also, in some cases the end of the vf episode is not annotated.\n",
    "\n",
    "In the other databases, all episodes start with \"+, (episode\",\n",
    "so the cudb files will be parsed so that they are consistent\n",
    "if more databases are used going forward.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CUDB_HZ = 250\n",
    "CUDB_EPISODES = ['(AF', '(VF', '(VT', '(N']\n",
    "\n",
    "data_dir = 'data\\physionet_dbs\\cudb\\\\annotations'\n",
    "write_data_dir = 'data\\physionet_dbs\\cudb\\\\annotations_systemized'\n",
    "if not os.path.exists(write_data_dir):\n",
    "    os.makedirs(write_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu01.ann\n",
      "(206, 6)\n",
      "cu02.ann\n",
      "(970, 6)\n",
      "cu03.ann\n",
      "(941, 6)\n",
      "cu04.ann\n",
      "(248, 6)\n",
      "cu05.ann\n",
      "(697, 6)\n",
      "cu06.ann\n",
      "(453, 6)\n",
      "cu07.ann\n",
      "(377, 6)\n",
      "cu08.ann\n",
      "(1209, 6)\n",
      "cu09.ann\n",
      "(925, 6)\n",
      "cu10.ann\n",
      "(557, 6)\n",
      "cu11.ann\n",
      "(508, 6)\n",
      "cu12.ann\n",
      "(410, 6)\n",
      "cu13.ann\n",
      "(870, 6)\n",
      "cu14.ann\n",
      "(534, 6)\n",
      "cu15.ann\n",
      "(284, 6)\n",
      "cu16.ann\n",
      "(836, 6)\n",
      "cu17.ann\n",
      "(533, 6)\n",
      "cu18.ann\n",
      "(688, 6)\n",
      "cu19.ann\n",
      "(679, 6)\n",
      "cu20.ann\n",
      "(208, 6)\n",
      "cu21.ann\n",
      "(647, 6)\n",
      "cu22.ann\n",
      "(440, 6)\n",
      "cu23.ann\n",
      "(443, 6)\n",
      "cu24.ann\n",
      "(511, 6)\n",
      "cu25.ann\n",
      "(559, 6)\n",
      "cu26.ann\n",
      "(773, 6)\n",
      "cu27.ann\n",
      "(1065, 6)\n",
      "cu28.ann\n",
      "(385, 6)\n",
      "cu29.ann\n",
      "(543, 6)\n",
      "cu30.ann\n",
      "(118, 6)\n",
      "cu31.ann\n",
      "(262, 6)\n",
      "cu32.ann\n",
      "(780, 6)\n",
      "cu33.ann\n",
      "(549, 6)\n",
      "cu34.ann\n",
      "(255, 6)\n",
      "cu35.ann\n",
      "(329, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    print(str(file).replace('.txt',''))\n",
    "\n",
    "    dataset = pd.read_table(f'{data_dir}/{file}', header=None, index_col=False, engine='python', sep = '[\\s]{2,}')\n",
    "    print(dataset.shape)\n",
    "\n",
    "    # columns and dtypes\n",
    "    dataset.columns = ['timestamp','sample_idx', 'beat', 'misc_1', 'misc_2','episode']\n",
    "    dataset.drop(labels = ['timestamp', 'misc_1', 'misc_2'], axis = 1, inplace = True)\n",
    "    dataset['beat'] = dataset['beat'].astype(str)\n",
    "    dataset['episode'] = dataset['episode'].astype(str)\n",
    "\n",
    "    # so that 0s in sample_idx are not replaced by the following step\n",
    "    sample_idx_column = dataset['sample_idx']\n",
    "    dataset.drop(labels = ['sample_idx'], inplace = True, axis = 1)\n",
    "\n",
    "    # handling problematic parsing\n",
    "    dataset.replace({0: '', '0': ''}, inplace = True)\n",
    "    for episode_iter in CUDB_EPISODES:\n",
    "        dataset.replace({f'0\\t{episode_iter}': f'{episode_iter}'}, inplace = True)\n",
    "    dataset.replace({'': None}, inplace = True)\n",
    "\n",
    "    # recover sample_idx column\n",
    "    dataset.insert(0, 'sample_idx', sample_idx_column)\n",
    "    dataset['sample_idx'] = dataset['sample_idx'].astype('int32')\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # CORRECTION 1: Removing VFL redundant brackets '[' and ']'\n",
    "\n",
    "    # cannot just delete them because the start of the next episode needs to be specified if it is not\n",
    "    # dataset = dataset[dataset['beat']!= ']']\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Case 1 for [: if there is an explicitly annotated VF episode, remove the [\n",
    "\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "    row_idxs_to_remove = list()\n",
    "\n",
    "    for row_idx, row in dataset.iterrows():\n",
    "\n",
    "        if row['beat'] == '[':\n",
    "\n",
    "            # if not first and last row\n",
    "            if row_idx != 0 and row_idx != dataset.shape[0] - 1:\n",
    "\n",
    "                # if (VF precedes or follows a [\n",
    "                if dataset.iloc[row_idx-1]['episode'] == '(VF' or dataset.iloc[row_idx+1]['episode'] == '(VF':\n",
    "                    row_idxs_to_remove.append(False); continue\n",
    "\n",
    "            elif row_idx == 0:\n",
    "                if dataset.iloc[row_idx+1]['episode'] == '(VF':\n",
    "                    row_idxs_to_remove.append(False); continue\n",
    "\n",
    "            else:\n",
    "                if dataset.iloc[row_idx-1]['episode'] == '(VF':\n",
    "                    row_idxs_to_remove.append(False); continue\n",
    "\n",
    "        # every other case\n",
    "        row_idxs_to_remove.append(True)\n",
    "\n",
    "    dataset = dataset[row_idxs_to_remove]\n",
    "\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Case 2 for [: if there isn't an explicitly annotated VF episode, replace the [ with +, VF\n",
    "\n",
    "    for row_idx, row in dataset.iterrows():\n",
    "            if row['beat'] == '[':\n",
    "                dataset.loc[row_idx, 'beat'] = '+'\n",
    "                dataset.loc[row_idx, 'episode'] = '(VF'\n",
    "\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Case 1 for ]: if there is an episode following the VF episode, remove the ]\n",
    "\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "    row_idxs_to_remove = list()\n",
    "\n",
    "    for row_idx, row in dataset.iterrows():\n",
    "\n",
    "        # assuming ] cannot be found in a first row\n",
    "        if row_idx == 0:\n",
    "            row_idxs_to_remove.append(True)\n",
    "            continue\n",
    "\n",
    "        if row['beat'] == ']':\n",
    "            if row_idx == dataset.shape[0] - 1 or dataset.iloc[row_idx+1]['episode'] in CUDB_EPISODES:\n",
    "                row_idxs_to_remove.append(False); continue\n",
    "\n",
    "        # every other case\n",
    "        row_idxs_to_remove.append(True)\n",
    "\n",
    "    dataset = dataset[row_idxs_to_remove]\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Case 2 for ]: if there isn't an episode following the VF episode, assume (N\n",
    "    for row_idx, row in dataset.iterrows():\n",
    "            if row['beat'] == ']':\n",
    "                dataset.loc[row_idx, 'beat'] = '+'\n",
    "                dataset.loc[row_idx, 'episode'] = '(N'\n",
    "\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # CORRECTION 2: Signal quality annotation change from '~, ' to '~, NOISE'\n",
    "\n",
    "    for row_idx, row in dataset.iterrows():\n",
    "        if row['beat'] == '~':\n",
    "            dataset.loc[row_idx, 'episode'] = 'NOISE'\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # CORRECTION 3: Ensure explicit episode annotation at begining of file\n",
    "\n",
    "    if dataset.iloc[0]['episode'] is None:\n",
    "        dataset = dataset.append(pd.DataFrame(data = [[0,'+','(N']], columns = ['sample_idx', 'beat', 'episode']))\n",
    "        dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    if dataset.iloc[0]['beat']=='~':\n",
    "        dataset = dataset.append(pd.DataFrame(data = [[0,'+','(N']], columns = ['sample_idx', 'beat', 'episode']))\n",
    "        dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    dataset.sort_values(by = ['sample_idx'], inplace = True)\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    dataset.to_csv(f'{write_data_dir}\\\\{str(file.replace(\".ann\",\"\"))}.csv', index = False)\n",
    "\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}